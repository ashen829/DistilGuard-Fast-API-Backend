"""
Chatbot Router - Integrates chatbot_app endpoints with DistilGuard

This module exposes the chatbot API endpoints from chatbot_app.
Routes are mounted at /api/chatbot/* prefix in the main application.
"""

import logging
from typing import List
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chatbot", tags=["chatbot"])


# ============================================================================
# Data Models (matching chatbot_app models)
# ============================================================================

class Message(BaseModel):
    """Chat message model"""
    role: str  # "user" or "assistant"
    content: str


class ChatRequest(BaseModel):
    """Chat request model"""
    message: str
    chat_history: List[Message] = []


class ChatResponse(BaseModel):
    """Chat response model"""
    response: str


# ============================================================================
# Chat Endpoints
# ============================================================================

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    Chat endpoint for asking questions about the FL detection data.
    
    The chatbot can answer questions like:
    - "How many clients are there?"
    - "Which clients were detected as malicious?"
    - "Tell me about client 3"
    - "What is the average confidence?"
    - Any other question about the detection dataset
    
    Args:
        request: ChatRequest with message and optional chat history
    
    Returns:
        ChatResponse with agent's answer
    
    Raises:
        HTTPException: 500 if error during processing
    """
    try:
        from chatbot_app.llm.agent import get_agent
        
        agent = get_agent()
        
        # Convert Message objects to dict format for agent
        history = [{"role": msg.role, "content": msg.content} for msg in request.chat_history]
        
        # Process through agent (analyzes CSV/JSON + LLM)
        response = agent.process(request.message, history)
        
        logger.info(f"Chatbot: '{request.message}' â†’ {len(response)} chars response")
        return ChatResponse(response=response)
        
    except Exception as e:
        logger.error(f"Error in /api/chatbot/chat endpoint: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stream")
async def stream(message: str):
    """
    Streaming chat endpoint using Server-Sent Events (SSE).
    
    Streams tokens as they're generated by the model for better UX.
    Same functionality as /chat but with token-by-token streaming.
    
    Args:
        message: The user's question
    
    Returns:
        StreamingResponse with SSE-formatted token stream
    """
    async def event_generator():
        try:
            from chatbot_app.llm.agent import get_agent
            
            agent = get_agent()
            
            # Stream tokens from agent
            for token in agent.stream_process(message, chat_history=[]):
                yield f"data: {token}\n\n"
            
            # Send completion signal
            yield "data: [DONE]\n\n"
            logger.info(f"Stream completed for: '{message}'")
            
        except Exception as e:
            logger.error(f"Error in /api/chatbot/stream endpoint: {str(e)}", exc_info=True)
            yield f"data: Error: {str(e)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get("/health")
async def health_check():
    """
    Health check endpoint for chatbot service.
    
    Validates that core components are initialized:
    - Agent is available
    - At least one data source (JSON or CSV) is loaded
    
    Returns 200 OK if healthy, 500 if critical components missing.
    
    Returns:
        dict with status and component health
    """
    try:
        from chatbot_app.llm.agent import get_agent
        
        agent = get_agent()
        
        # Check if JSON data is available
        json_available = False
        if agent.json_analyzer and agent.json_analyzer.data:
            json_available = True
        
        # Check if CSV data is available
        csv_available = False
        if agent.csv_analyzer and agent.csv_analyzer.df is not None:
            csv_available = True
        
        # At least one data source must be available
        if not (json_available or csv_available):
            raise Exception("No data source available (JSON and CSV both missing)")
        
        data_source = "JSON" if json_available else "CSV"
        
        return {
            "status": "healthy",
            "message": "Chatbot service is running",
            "agent": "ready",
            "data_source": data_source,
            "json_available": json_available,
            "csv_available": csv_available
        }
    except Exception as e:
        logger.error(f"Chatbot health check failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Chatbot health check failed: {str(e)}"
        )
